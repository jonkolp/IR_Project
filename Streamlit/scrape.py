import urllib.robotparser
import urllib.request
from urllib.error import URLError, HTTPError
from urllib.parse import quote_plus
import requests
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
import time
import random
import pandas as pd
import os

class AmazonContentExtractor:
    def __init__(self, base_url="https://www.amazon.com/s?i=electronics", keyword="tech", max_pages=3):
        self.base_url = base_url
        self.keyword = keyword
        self.max_pages = max_pages
        self.headers = {
            "User-Agent": UserAgent().random,
            "Accept-Language": "en-US,en;q=0.9"
        }

    def crawl(self):
        all_products = []
        for page in range(1, self.max_pages + 1):
            url = f"{self.base_url}&page={page}"
            print(f"üìÑ Crawling page {page}: {url}")
            response = requests.get(url, headers=self.headers)

            if response.status_code != 200:
                print(f"‚ùå Failed to fetch page {page}")
                continue

            soup = BeautifulSoup(response.content, "html.parser")
            products = self.extract_products(soup)
            all_products.extend(products)

            delay = random.uniform(2, 5)
            print(f"‚è≥ Sleeping for {delay:.2f} seconds")
            time.sleep(delay)

        return all_products

    def extract_products(self, soup):
        items = soup.select("div.s-result-item")
        results = []

        for item in items:
            try:
                name = item.select_one("h2 span") and item.select_one("h2 span").text.strip()
                price_whole = item.select_one(".a-price .a-price-whole")
                price_fraction = item.select_one(".a-price .a-price-fraction")
                price = None
                if price_whole and price_fraction:
                    price = f"${price_whole.text.strip()}.{price_fraction.text.strip()}"

                rating = item.select_one(".a-icon-alt")
                customer_ratings = rating.text.strip() if rating else None

                image = item.select_one("img.s-image")
                image_url = image["src"] if image else None

                if name:
                    results.append({
                        "name": name,
                        "price": price,
                        "customer_ratings": customer_ratings,
                        "category": self.keyword,
                        "image": image_url
                    })

            except Exception as e:
                print(f"‚ö†Ô∏è Skipped item due to error: {e}")
                continue

        return results

def save_to_csv(products, filepath):
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    df = pd.DataFrame(products)
    df.to_csv(filepath, index=False)
    print(f"‚úÖ CSV saved to: {filepath}")

def save_to_excel(products, filepath):
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    df = pd.DataFrame(products)
    df.to_excel(filepath, index=False)
    print(f"‚úÖ Excel file saved to: {filepath}")


tech_keywords = [
    "smart home devices", "laptops", "wireless chargers",
    "smartphones", "Bluetooth speakers", "wireless earbuds",
    "gaming accessories", "4K monitors"
]

all_products = []

for keyword in tech_keywords:
    print(f"\nüîç Searching for: {keyword}")
    search_url = f"https://www.amazon.com/s?k={keyword.replace(' ', '+')}"
    scraper = AmazonContentExtractor(base_url=search_url, keyword=keyword, max_pages=2)
    products = scraper.crawl()

    print(f"üßÆ Found {len(products)} products for '{keyword}'")

    for i, product in enumerate(products[:5], 1):
        print(f"\n--- Product {i} ---")
        for key, value in product.items():
            print(f"{key}: {value}")

    all_products.extend(products)

# Save with custom path
save_to_csv(all_products, "Streamlit/tech_products.csv")
save_to_excel(all_products, "Streamlit/tech_products.xlsx")